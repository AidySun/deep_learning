
## Activation Function
* ReLU (Rectified Linear Unit)
  `a = max(0, z)`
* tanh
  `a = (e[z] - e[-z]) / (e[z] + e[-z])`
* Leaky ReLU
  `a = max(tiny * z, z)`
* sigmoid
  `a = 1 / (1 + e[-z])`
